{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 30px;\">\n",
    "  Submitted By\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 30px; color: green; font-weight: bold;\">\n",
    "  Name - Dipendu Ghosh\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 30px; color: red; font-weight: bold;\">\n",
    "  Roll No- CS23M509\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CS6650W Homework 1 (Diwali 2024 Semester)\n",
    "#### Smartphone based Photoplethysmography (PPG)\n",
    "The task is to develop a PPG system using a smartphone device to capture blood flow related imagery data and post-process such data to estimate the pulse or heart beat rate of the subject. You need to implement various features pertaining to the above task in this notebook. Create a directory, <b><your_roll>_CS6650D24</b>. Place this notebook in that directory.\n",
    "\n",
    "#### A. Warmup - Data Collection [10 points]\n",
    "Use your smartphone’s camera to capture the blood-flow video. Turn on the flash while recording and turn it off once done. Capture the video using the maximum frame rate that is available in your phone (e.g.60 fps or 30 fps). The same holds for image resolution. The captured videos are saved on the device’s SD card. Collect 3 such videos (each exactly 10 seconds long, if longer just limit to N frames while processing, where N = 10*frames_per_second) under the following conditions. Make sure that all video capture configurations are kept the same across the cases (resolution, fps).\n",
    "* resting on bed (1.mp4)\n",
    "* after a moderate walk (2.mp4), and\n",
    "* after a vigorous exercise (3.mp4).\n",
    "\n",
    "Copy these videos locally to your project folder in a subdirectory \"ppgvideos\" - name them (1/2/3).mp4. Don’t move your finger/hand randomly or press too hard against the camera or flash while recording, little randomness is okay. Write a script to read the three videos and store the frames (2D vector of (R G B) values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Each Import:\n",
    "\n",
    "1. **OpenCV (cv2)**: This library is primarily used for image and video processing tasks, including reading, writing, and manipulating video frames and images.\n",
    "\n",
    "2. **NumPy (np)**: A fundamental package for numerical computing in Python, it provides support for arrays, matrices, and a variety of mathematical functions to operate on these data structures.\n",
    "\n",
    "3. **Matplotlib (plt)**: A plotting library that enables the creation of static, animated, and interactive visualizations. The pyplot module (plt) is used for creating plots.\n",
    "\n",
    "4. **Random**: A module that implements pseudo-random number generators for various distributions, useful for creating random selections, shuffling data, or generating random numbers.\n",
    "\n",
    "5. **SciPy Signal (find_peaks)**: This function is used to identify peaks in a one-dimensional array of data, which is particularly useful in signal processing and data analysis.\n",
    "\n",
    "6. **Seaborn (sns)**: A data visualization library based on Matplotlib that provides a high-level interface for drawing attractive statistical graphics. It simplifies the creation of complex visualizations.\n",
    "\n",
    "7. **SciPy Stats (norm)**: This class is used for statistical operations, especially for fitting data to a normal distribution and performing statistical analysis.\n",
    "\n",
    "These comments will help other developers understand the purpose of each library and its role in the project, enhancing code readability and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenCV library for image and video processing tasks\n",
    "import cv2\n",
    "\n",
    "# Import NumPy for numerical operations and array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Import Matplotlib for plotting graphs and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the random module to generate random numbers and selections\n",
    "import random\n",
    "\n",
    "# Import find_peaks function from SciPy's signal module to detect peaks in data\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Import seaborn for advanced data visualization and statistical plotting\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the norm class from scipy.stats for statistical operations, particularly for fitting distributions\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Video Paths and Frame Storage**:\n",
    "   - videoPaths: A list of strings representing the file paths of the videos to be processed.\n",
    "   - videoFrames: A dictionary used to store the frames of each video. The keys (\"1\", \"2\", \"3\") represent the video IDs.\n",
    "\n",
    "2. **Video Properties Storage**:\n",
    "   - framesPerSecondValues, frameCountValues, frameHeightValues, frameWidthValues: These dictionaries store the FPS (Frames per second), total frame count, frame height, and frame width for each video, respectively.\n",
    "\n",
    "3. **Video Processing Loop**:\n",
    "   - **cv2.VideoCapture(videoPath)**: Opens the video file for reading.\n",
    "   - The OpenCV CAP_PROP_* constants are used to retrieve the video's FPS, total number of frames, and dimensions (height and width).\n",
    "   - A loop reads all frames from the video using cap.read(). If a frame is successfully read (ret is True), it is added to the videoFrames dictionary.\n",
    "   - The loop continues until no more frames can be read (end of video), at which point the video capture object is released, and any OpenCV windows are closed.\n",
    "\n",
    "4. **Logging Video Properties**:\n",
    "   - After processing each video, the video path, total frame count, FPS, frame height, and frame width are printed for reference.\n",
    "\n",
    "5. **Asumption**:\n",
    "   - All videos are in the same orientation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## your snippet to read the three videos, display the number of frames and resolution in each video\n",
    "\n",
    "# List of paths to the video files\n",
    "videoPaths = [\"ppgvideos/1.mp4\", \"ppgvideos/2.mp4\", \"ppgvideos/3.mp4\"]\n",
    "\n",
    "# Dictionary to store the frames of each video (each key corresponds to a video ID)\n",
    "videoFrames = {\"1\": [], \"2\": [], \"3\": []}\n",
    "\n",
    "# Dictionaries to store video properties (FPS, frame count, frame dimensions)\n",
    "framesPerSecondValues = {\"1\": 0, \"2\": 0, \"3\": 0}  # Frames per second for each video\n",
    "frameCountValues = {\"1\": 0, \"2\": 0, \"3\": 0}       # Total frame count for each video\n",
    "frameHeightValues = {\"1\": 0, \"2\": 0, \"3\": 0}      # Frame height for each video\n",
    "frameWidthValues = {\"1\": 0, \"2\": 0, \"3\": 0}       # Frame width for each video\n",
    "\n",
    "# Loop through each video in the list of video paths\n",
    "for index, videoPath in enumerate(videoPaths):\n",
    "    \n",
    "    # Open the video file for reading\n",
    "    cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "    # Retrieve video properties using OpenCV's CAP_PROP constants\n",
    "    framesPerSecond = cap.get(cv2.CAP_PROP_FPS)             # Frames per second of the video\n",
    "    frameCount = cap.get(cv2.CAP_PROP_FRAME_COUNT)          # Total number of frames in the video\n",
    "    frameWidth = cap.get(cv2.CAP_PROP_FRAME_WIDTH)          # Width of each video frame\n",
    "    frameHeight = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)        # Height of each video frame\n",
    "\n",
    "    # Store the video properties in the corresponding dictionaries using the video index as a key\n",
    "    framesPerSecondValues[str(index + 1)] = framesPerSecond  # Store FPS for the current video\n",
    "    frameCountValues[str(index + 1)] = frameCount            # Store total frame count\n",
    "    frameHeightValues[str(index + 1)] = frameHeight          # Store frame height\n",
    "    frameWidthValues[str(index + 1)] = frameWidth            # Store frame width\n",
    "    \n",
    "    # Loop to read all frames from the video until no more frames are available\n",
    "    while cap.isOpened():\n",
    "        ret, frameTemp = cap.read()  # Read the next frame from the video\n",
    "        if ret:  # If a frame is successfully read\n",
    "            # Add the frame to the list of frames for the current video\n",
    "            videoFrames[str(index + 1)].append(frameTemp)\n",
    "        else:\n",
    "            # If no frame is returned, end the loop (video has ended)\n",
    "            break\n",
    "    \n",
    "    # Release the video capture object and close any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print the video details for reference\n",
    "    print(f\"{videoPath}, Frame Count: {frameCount}, FPS: {framesPerSecond}, Height: {frameHeight}, Width: {frameWidth}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### B. Sensing Metric [5 points]\n",
    "Design your sensing metric. Note that each frame is a 2D vector of size [AxB], containing A.B pixels, where a pixel at location [i,j] is denoted by the 3-tuple [B, G, R] where B, G, and R are bytes (8 bits, range 0 - 255) representing intensity of each color - Blue, Green and Red. The frame intensity metric is an aggregate statistical measure on the pixel values. (you can even consider R, G and B streams separately or consider greyscale frames). Best to have a normalized value between zero and one. [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **calculate_intensity_metric Function**:\n",
    "   - **Input**: Takes in the videoIndex (to track which video is being processed) and a frame (which is a multi-channel image, usually in BGR format).\n",
    "   - **Process**: \n",
    "     - Splits the frame into its Blue, Green, and Red components using cv2.split().\n",
    "     - Calculates the average intensity for each channel using np.mean().\n",
    "     - Normalizes each channel's intensity by dividing by 255, which is the maximum pixel value for an 8-bit image.\n",
    "     - Combines the normalized intensity values using predefined weights (red has the highest weight).\n",
    "   - **Output**: Returns a frameIntensityMetric, which is a single scalar value representing the intensity of the frame.\n",
    "\n",
    "2. **Intensity Metrics Storage**:\n",
    "   - A dictionary intensityMetrics is initialized to store intensity metrics for each video (\"1\", \"2\", \"3\" are the keys corresponding to video IDs).\n",
    "   - For each video and each frame within that video, the calculate_intensity_metric function is called to compute the frame's intensity.\n",
    "   - The result is stored as a dictionary containing the frame index and the calculated intensity metric.\n",
    "\n",
    "3. **Looping Through Videos and Frames**:\n",
    "   - The outer loop iterates over the videoFrames dictionary, where videoIndex is the key representing the video ID and frames is the list of frames in that video.\n",
    "   - The inner loop processes each frame and calculates its intensity metric, storing the result in intensityMetrics.\n",
    "\n",
    "4. **Optional Debugging Print**:\n",
    "   - The print() statement is commented out but can be used to display the intensity metric for each frame during runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## put your snippet here with complete explanation in comments\n",
    "## Marks will be provided based on code readability (not yourself, but others) and detailed comments\n",
    "\n",
    "def calculate_intensity_metric(frame):\n",
    "    \"\"\"\n",
    "    Calculates the Frame Intensity Metric for a given video frame.\n",
    "    \n",
    "    Args:\n",
    "    - videoIndex (str): The index of the video being processed.\n",
    "    - frame (numpy array): The current video frame represented as a multi-dimensional array.\n",
    "    \n",
    "    Returns:\n",
    "    - frameIntensityMetric (float): The calculated intensity metric for the frame.\n",
    "    \"\"\"\n",
    "    # Split the input frame into three color channels: Blue, Green, and Red\n",
    "    blue, green, red = cv2.split(frame)\n",
    "\n",
    "    # Calculate the average intensity for each color channel\n",
    "    averageIntensityRed = np.mean(red)\n",
    "    averageIntensityGreen = np.mean(green)\n",
    "    averageIntensityBlue = np.mean(blue)\n",
    "    \n",
    "    # Normalize the calculated average intensities by dividing by 255 (max value for 8-bit images)\n",
    "    normalizedIntensityRed = averageIntensityRed / 255.0\n",
    "    normalizedIntensityGreen = averageIntensityGreen / 255.0\n",
    "    normalizedIntensityBlue = averageIntensityBlue / 255.0\n",
    "\n",
    "    # Define weights for combining the normalized color intensities\n",
    "    # Here, the red channel is given the most importance (weight 0.8), \n",
    "    # and the green and blue channels are weighted equally at 0.1.\n",
    "    weightRed, weightGreen, weightBlue = 0.8, 0.1, 0.1\n",
    "    \n",
    "    # Compute the overall intensity metric by combining the weighted normalized color intensities\n",
    "    frameIntensityMetric = (weightRed * normalizedIntensityRed + \n",
    "                            weightGreen * normalizedIntensityGreen + \n",
    "                            weightBlue * normalizedIntensityBlue)\n",
    "    \n",
    "    return frameIntensityMetric\n",
    "\n",
    "# Initialize a dictionary to store the intensity metrics for each video\n",
    "intensityMetrics = {\"1\": [], \"2\": [], \"3\": []}\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Main processing loop for video frames\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Loop through each video in the videoFrames dictionary\n",
    "for videoIndex, frames in videoFrames.items():\n",
    "    # Loop through each frame in the current video\n",
    "    for frameIndex, frameTemp in enumerate(frames):\n",
    "        # Calculate the intensity metric for the current frame\n",
    "        intensityMetric = calculate_intensity_metric(frameTemp)\n",
    "        \n",
    "        # Store the intensity metric along with the frame index in the intensityMetrics dictionary\n",
    "        intensityMetrics[str(videoIndex)].append({\n",
    "            'frameIndex': frameIndex + 1,  # Frame index starts from 1 for clarity\n",
    "            'intensityMetric': intensityMetric\n",
    "        })\n",
    "        \n",
    "        # Comment/Uncomment the line below if you want to print/not print the intensity metric for each frame\n",
    "        print(f\"Video {videoIndex}, Frame {frameIndex + 1}, Intensity Metric: {intensityMetric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### C. Temporal Variation of Intensity Value [10 points]\n",
    "Plot your frame intensity metric vs. time for a random 5-second chunk of the three videos. The X-axis should be common for all three subplots (stacked vertically) with separate Y-axes based on your intensity metric. Appreciate the fact that vigorous exercise leads to rapid intensity variations compared to while resting. What is the BPM value for the three cases (manually counting is okay)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - **chunkInfos**: Stores information about the chunks extracted from each video.\n",
    "   - **chunks**: A dictionary to hold the actual intensity data chunks from each video.\n",
    "   - **timeAxes**: Stores the time axes corresponding to the chunks for plotting purposes.\n",
    "   - **plotColors**: Defines the colors to use for plotting each video’s intensity metrics for better visualization.\n",
    "   - **bpmValues**: Holds the estimated BPM for each video based on detected peaks.\n",
    "\n",
    "2. **get_random_chunk Function**:\n",
    "   - This function extracts a random 5-second chunk of frames from a video.\n",
    "   - It calculates the chunk size based on the frames per second (fps).\n",
    "   - A random starting frame index is selected to ensure a complete chunk can be extracted.\n",
    "   - The extracted chunk of intensity values is returned for further processing.\n",
    "\n",
    "3. **Extracting Chunks**:\n",
    "   - The main loop iterates through the intensityMetrics, extracting random chunks of intensity data for each video.\n",
    "   - A corresponding time axis is generated for each chunk, ranging from 0 to 5 seconds.\n",
    "\n",
    "4. **BPM Calculation**:\n",
    "   - The find_peaks function detects peaks in the intensity values, and the total number of peaks is counted.\n",
    "   - BPM is estimated by multiplying the number of peaks found in the 5-second chunk by 12.\n",
    "\n",
    "5. **Output BPM Values**:\n",
    "   - The estimated BPM for each video is printed to the console.\n",
    "\n",
    "6. **Plotting**:\n",
    "   - A 3x1 subplot is created to visualize the intensity metrics over time for each video.\n",
    "   - The intensity values are plotted against the time axes.\n",
    "   - Detected peaks are marked on the plots.\n",
    "   - The layout is adjusted to ensure plots do not overlap, and the resulting plots are displayed. \n",
    "\n",
    "7. **Assumption**:\n",
    "   - The chunk will be dynamically chosen on each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## put your snippet here with complete explanation in comments\n",
    "## Marks will be provided based on code readability (not yourself, but others) and detailed comments\n",
    "\n",
    "def get_random_chunk(frames, videoIndex, fps, duration):\n",
    "    \"\"\"\n",
    "    Extracts a random chunk of frames from a video.\n",
    "\n",
    "    Args:\n",
    "    - frames (list): List of frames (intensity metrics) from the video.\n",
    "    - videoIndex (str): The index of the video being processed.\n",
    "    - fps (float): Frames per second of the video.\n",
    "    - duration (int): Duration of the chunk in seconds (default is 5 seconds).\n",
    "\n",
    "    Returns:\n",
    "    - List of intensity metrics for the randomly selected chunk of frames.\n",
    "    \"\"\"\n",
    "    # Calculate the number of frames in the chunk based on the given duration\n",
    "    chunkSize = int(fps * duration)\n",
    "    \n",
    "    # Ensure there are enough frames to select a random chunk\n",
    "    if len(frames) < chunkSize:\n",
    "        raise ValueError(f\"Video {videoIndex} has fewer frames than needed for a {duration}-second chunk.\")\n",
    "    \n",
    "    # Select a random starting frame index to extract a full 5-second chunk\n",
    "    startFrame = random.randint(0, len(frames) - chunkSize)\n",
    "    \n",
    "    # Store chunk size and starting frame in the chunkInfos dictionary for reference\n",
    "    chunkInfos[str(videoIndex)] = {'chunkSize': chunkSize, 'startFrame': startFrame}\n",
    "\n",
    "    # Return the extracted chunk of intensity values from the intensityMetrics\n",
    "    return intensityMetrics[videoIndex][startFrame:startFrame + chunkSize], chunkInfos\n",
    "\n",
    "def plot_intensity_metric_bpm():\n",
    "    \"\"\"\n",
    "    Plots intensity metrics over time for multiple videos and marks the peaks detected \n",
    "    in the intensity data. This function is designed to visualize the temporal variation \n",
    "    of intensity metrics and highlight significant peaks, typically used to analyze \n",
    "    variations like heart rate.\n",
    "\n",
    "    The plot contains subplots for each video, where:\n",
    "    - The x-axis represents time in seconds.\n",
    "    - The y-axis represents the intensity metric.\n",
    "    - Peaks in intensity are marked with 'x' symbols.\n",
    "    - A title for each subplot shows the corresponding video and BPM value.\n",
    "\n",
    "    Outputs:\n",
    "    - Displays the intensity metrics plot with peaks per video.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store plot colors for each video for visualization purposes\n",
    "    plotColors = {\"1\": 'blue', \"2\": 'green', \"3\": 'red'}\n",
    "\n",
    "    # Iterate over each video to plot intensity metrics and mark detected peaks\n",
    "    for _, videoIndex in enumerate(intensityMetrics):\n",
    "        # Create a figure with 3 subplots (one for each video)\n",
    "        fig, ax = plt.subplots(figsize=(15, 3))\n",
    "\n",
    "        # Extract intensity values from the chunks data for the current video\n",
    "        intensityValues = [frameData['intensityMetric'] for frameData in chunks[videoIndex]]\n",
    "        \n",
    "        # Plot intensity values over time for the current video\n",
    "        ax.plot(timeAxes[videoIndex], intensityValues, color=plotColors[videoIndex], label=f\"Video {videoIndex}\")\n",
    "        \n",
    "        # Detect peaks in the intensity data using find_peaks, based on frame rate (0.5-second minimum distance)\n",
    "        peaks, _ = find_peaks(intensityValues, distance=framesPerSecondValues[videoIndex] * 0.4)\n",
    "        \n",
    "        # Mark the detected peaks on the plot with black 'x' markers\n",
    "        ax.plot(timeAxes[videoIndex][peaks], np.array(intensityValues)[peaks], \"x\", color='black', label=\"Peaks\", markersize=10, markeredgewidth=2)\n",
    " \n",
    "        # Set the x-axis label for each subplot to indicate the time\n",
    "        ax.set_xlabel(f\"Time (seconds)\")\n",
    "\n",
    "        # Set the y-axis label for each subplot to indicate the intensity metric\n",
    "        ax.set_ylabel(f\"Intensity (Video {videoIndex})\")\n",
    "        \n",
    "        # Set the title for each subplot, including the video index and its corresponding BPM value\n",
    "        ax.set_title(f\"Video {videoIndex} - Intensity vs. Time (BPM: {bpmValues[videoIndex]})\")\n",
    "        \n",
    "        # Display a legend showing the plot labels (video number and peaks)\n",
    "        ax.legend()\n",
    "\n",
    "        # Adjust the layout to ensure the subplots don't overlap and display the final plot\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize a dictionary to store chunk information for each video\n",
    "chunkInfos = {\"1\": [], \"2\": [], \"3\": []}\n",
    "\n",
    "# Create dictionaries to store the extracted chunks\n",
    "chunks = {\"1\": [], \"2\": [], \"3\": []}\n",
    "# Time axes for each video\n",
    "timeAxes = {\"1\": [], \"2\": [], \"3\": []}\n",
    "\n",
    "# Dictionary to store estimated BPM (beats per minute) values for each video\n",
    "bpmValues = {\"1\": 0, \"2\": 0, \"3\": 0}\n",
    "\n",
    "# Duration of each chunk. This can be modified as per requirement\n",
    "duration = 5\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Main processing loop for video frames\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Extract random 5-second chunks from the intensity metrics for each video\n",
    "for videoIndex, frames in intensityMetrics.items():\n",
    "    # Get a random chunk of intensity metrics for the current video\n",
    "    chunks[videoIndex], chunkInfos = get_random_chunk(frames, videoIndex, framesPerSecondValues[videoIndex], duration)\n",
    "\n",
    "    # Create a time axis for the extracted chunk, ranging from 0 to 5 seconds\n",
    "    timeAxes[videoIndex] = np.linspace(0, 5, len(chunks[videoIndex]))\n",
    "\n",
    "    # Extract intensity values from the chunk for peak detection\n",
    "    intensityValues = [frameData['intensityMetric'] for frameData in chunks[videoIndex]]\n",
    "    \n",
    "    # Find peaks in the intensity values to estimate BPM. The fps is multiplied by 0.5 so that all peaks close \n",
    "    # to the one selected are discarded and only something at a proper distance is taken. \n",
    "    # Why 0.5 sec? Assuming every half a second a the heart beats\n",
    "    peaks, _ = find_peaks(intensityValues, distance=framesPerSecondValues[videoIndex] * 0.4)\n",
    "    totalPeaks = len(peaks)\n",
    "    \n",
    "    # Estimate BPM based on the number of detected peaks in the 5-second chunk\n",
    "    bpmValues[videoIndex] = totalPeaks * 12  # Multiply by 12 to convert to BPM (60 seconds / 5 seconds)\n",
    "\n",
    "# Output the estimated BPM values for each video\n",
    "for videoIndex, bpm in bpmValues.items():\n",
    "    print(f\"Video {videoIndex} - Estimated BPM: {bpm}\")\n",
    "\n",
    "# Plot the intensity metrics over time for the three videos\n",
    "plot_intensity_metric_bpm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### D. Likelihood Distributions [20 marks]\n",
    "In the 5-second chunks taken above, choose 20 frames where your sensing metric is close to the local maximum (Case1), and 20 frames where it is close to the local minimum (Case2). Plot the histograms on \"R\", \"G\" and \"B\" values for each pixel in the 20 frames for the two cases 1 and 2. For each video there will be 3 figures, each for \"R\", \"G\" and \"B\". Which one produces the most separable distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **select_extreme_frames(intensityChunk, chunkInfo, videoIndex, numFrames=20)**\n",
    "   - **Purpose**: Selects frames with minimum and maximum intensity metrics from a given chunk of frames.\n",
    "   - **Parameters**: \n",
    "     - intensityChunk: List of intensity metrics.\n",
    "     - chunkInfo: Additional information about the chunk (not directly used).\n",
    "     - videoIndex: Index of the video being processed.\n",
    "     - numFrames: Number of frames to select near the extremes (default is 20).\n",
    "   - **Returns**: Lists of frames near the maximum and minimum intensities, along with their indices.\n",
    "\n",
    "2. **extract_rgb_values_histogram_1D(frames, videoIndex)**\n",
    "   - **Purpose**: Extracts the RGB values from selected frames and returns them as flattened arrays.\n",
    "   - **Parameters**: \n",
    "     - frames: List of frames to extract RGB values from.\n",
    "     - videoIndex: Index of the video being processed.\n",
    "   - **Returns**: Flattened arrays of red, green, and blue pixel intensities.\n",
    "\n",
    "3. **plot_local_min_max(minFrames, maxFrames, minIndices, maxIndices, intensityChunk, videoIndex)**\n",
    "   - **Purpose**: Plots the local minimum and maximum intensity metrics over time for a specific video.\n",
    "   - **Parameters**: \n",
    "     - minFrames: List of frames with minimum intensity metrics.\n",
    "     - maxFrames: List of frames with maximum intensity metrics.\n",
    "     - minIndices: Indices of the minimum intensity frames.\n",
    "     - maxIndices: Indices of the maximum intensity frames.\n",
    "     - intensityChunk: Intensity metrics for the current chunk.\n",
    "     - videoIndex: Index of the video being processed.\n",
    "   - **Returns**: None (displays a plot).\n",
    "\n",
    "4. **plot_histograms(redMax, greenMax, blueMax, redMin, greenMin, blueMin, videoIndex)**\n",
    "   - **Purpose**: Plots histograms of RGB intensity distributions for maximum and minimum frames.\n",
    "   - **Parameters**: \n",
    "     - redMax, greenMax, blueMax: Flattened arrays of RGB intensities from maximum frames.\n",
    "     - redMin, greenMin, blueMin: Flattened arrays of RGB intensities from minimum frames.\n",
    "     - videoIndex: Index of the video being processed.\n",
    "   - **Returns**: None (displays histograms).\n",
    "\n",
    "5. **Main Loop for Frame Selection and Plotting**\n",
    "   - **Purpose**: Iterates through each video to select frames near maxima and minima, extract RGB values, and plot relevant data.\n",
    "   - **Key Operations**: \n",
    "     - Retrieves intensity values for a chunk of frames.\n",
    "     - Calls select_extreme_frames to get maximum and minimum frames.\n",
    "     - Extracts RGB values using extract_rgb_values_histogram_1D.\n",
    "     - Calls plot_local_min_max and plot_histograms to visualize the data.\n",
    "\n",
    "### General Summary\n",
    "The provided code processes video frames by selecting extreme intensity values, extracting RGB pixel intensities, and plotting both the intensity over time and histograms of the RGB distributions for selected frames. It enables the analysis of intensity variations in the video, facilitating insights into the visual data captured. It seems the Green Channel has the most separable distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## put your snippet here with complete explanation in comments\n",
    "## Marks will be provided based on code readability (not yourself, but others) and detailed comments\n",
    "\n",
    "def select_extreme_frames(intensityChunk, numFrames=20):\n",
    "    \"\"\"\n",
    "    Selects frames with the minimum and maximum intensity metrics from a given intensity chunk.\n",
    "\n",
    "    Parameters:\n",
    "    - intensityChunk: List of intensity metrics for the chunk of frames.\n",
    "    - numFrames: Number of frames to select near the minimum and maximum intensity (default is 20).\n",
    "\n",
    "    Returns:\n",
    "    - maxFrames: List of frames with intensity values near the maximum.\n",
    "    - minFrames: List of frames with intensity values near the minimum.\n",
    "    - maxIndex: Index of the frame with the maximum intensity.\n",
    "    - minIndex: Index of the frame with the minimum intensity.\n",
    "    \"\"\"\n",
    "    # Find the frame with the minimum intensity metric and get its original index\n",
    "    minIndex, minPair = min(enumerate(intensityChunk), key=lambda x: x[1]['intensityMetric'])\n",
    "    minFrameIndex = minPair['frameIndex']\n",
    "\n",
    "    # Find the frame with the maximum intensity metric and get its original index\n",
    "    maxIndex, maxPair = max(enumerate(intensityChunk), key=lambda x: x[1]['intensityMetric'])\n",
    "    maxFrameIndex = maxPair['frameIndex']\n",
    "\n",
    "    # Select frames sorted by proximity to the minimum frame index\n",
    "    minFrames = sorted(intensityChunk, key=lambda x: abs(x['frameIndex'] - minFrameIndex))[:numFrames]\n",
    "\n",
    "    # Select frames sorted by proximity to the maximum frame index\n",
    "    maxFrames = sorted(intensityChunk, key=lambda x: abs(x['frameIndex'] - maxFrameIndex))[:numFrames]\n",
    "\n",
    "    return maxFrames, minFrames, maxIndex, minIndex\n",
    "\n",
    "def extract_rgb_values_histogram_1D(frames, videoIndex):\n",
    "    \"\"\"\n",
    "    Extracts the RGB values from selected frames and returns them as flattened arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - frames: List of frames to extract RGB values from.\n",
    "    - videoIndex: Index of the video being processed.\n",
    "\n",
    "    Returns:\n",
    "    - redValues: Flattened array of red pixel intensities.\n",
    "    - greenValues: Flattened array of green pixel intensities.\n",
    "    - blueValues: Flattened array of blue pixel intensities.\n",
    "    \"\"\"\n",
    "    # Check if there are no frames selected\n",
    "    if len(frames) == 0:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    redValues, greenValues, blueValues = [], [], []\n",
    "    totalBluePixels = 0\n",
    "    totalGreenPixels = 0\n",
    "    totalRedPixels = 0\n",
    "\n",
    "    # Loop through each selected frame to extract RGB values\n",
    "    for frame in frames:\n",
    "        frameIndex = frame['frameIndex']  # Get the index of the current frame\n",
    "        frameTemp = videoFrames[videoIndex][frameIndex]  # Retrieve the frame from the video\n",
    "\n",
    "        # Split the frame into its RGB components\n",
    "        blue, green, red = cv2.split(frameTemp)\n",
    "\n",
    "        # Get the dimensions of each color channel\n",
    "        blueHeight, blueWidth = blue.shape\n",
    "        greenHeight, greenWidth = green.shape\n",
    "        redHeight, redWidth = red.shape\n",
    "\n",
    "        # Count total pixels for each channel\n",
    "        totalBluePixels += blueHeight * blueWidth\n",
    "        totalGreenPixels += greenHeight * greenWidth\n",
    "        totalRedPixels += redHeight * redWidth\n",
    "\n",
    "        # Flatten and append the RGB values to their respective lists\n",
    "        redValues.append(red.ravel())\n",
    "        greenValues.append(green.ravel())\n",
    "        blueValues.append(blue.ravel())\n",
    "\n",
    "    # Return concatenated RGB values as flattened arrays\n",
    "    return np.concatenate(redValues), np.concatenate(greenValues), np.concatenate(blueValues)\n",
    "\n",
    "def plot_local_min_max(minIndices, maxIndices, intensityChunk, videoIndex):\n",
    "    \"\"\"\n",
    "    Plots the local minimum and maximum intensity metrics over time for a specific video.\n",
    "\n",
    "    Parameters:\n",
    "    - minFrames: List of frames with minimum intensity metrics.\n",
    "    - maxFrames: List of frames with maximum intensity metrics.\n",
    "    - minIndices: Indices of the minimum intensity frames.\n",
    "    - maxIndices: Indices of the maximum intensity frames.\n",
    "    - intensityChunk: Intensity metrics for the current chunk.\n",
    "    - videoIndex: Index of the video being processed.\n",
    "\n",
    "    Outputs:\n",
    "    - Displays the intensity metrics plot with local minimum and maximum for R, G, B per video.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store plot colors for each video for visualization purposes\n",
    "    plotColors = {\"1\": 'blue', \"2\": 'green', \"3\": 'red'}\n",
    "    \n",
    "    # Create a new figure and axis for the plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 2))\n",
    "\n",
    "    # Plot the intensity metrics over time for the current video\n",
    "    intensityValues = [frameData['intensityMetric'] for frameData in intensityChunk]\n",
    "    ax.plot(timeAxes[videoIndex], intensityValues, color=plotColors[videoIndex], label=f\"Video {videoIndex}\")\n",
    "\n",
    "    # Mark the locations of the min and max intensity points\n",
    "    ax.plot(timeAxes[videoIndex][[minIndices[videoIndex], maxIndices[videoIndex]]], \n",
    "        np.array(intensityValues)[[minIndices[videoIndex], maxIndices[videoIndex]]], \n",
    "        \"x\", color='black', label=\"Peaks\", markersize=10, markeredgewidth=2)\n",
    "\n",
    "    # Set Y-axis label for intensity\n",
    "    ax.set_ylabel(f\"Intensity (Video {videoIndex})\")\n",
    "\n",
    "    # Set X-axis label for time\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "\n",
    "    # Set title for the plot\n",
    "    ax.set_title(f\"Video {videoIndex} - Local Minimum Maximum\")\n",
    "\n",
    "    # Enable legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_histograms(redMax, greenMax, blueMax, redMin, greenMin, blueMin, videoIndex):\n",
    "    \"\"\"\n",
    "    Plots histograms of RGB intensity distributions for maximum and minimum frames \n",
    "    of a video. Each RGB channel's histogram is plotted separately, showing the \n",
    "    distribution of intensities for both maximum and minimum frames.\n",
    "\n",
    "    Parameters:\n",
    "    - redMax: Flattened array of red intensities from maximum frames.\n",
    "    - greenMax: Flattened array of green intensities from maximum frames.\n",
    "    - blueMax: Flattened array of blue intensities from maximum frames.\n",
    "    - redMin: Flattened array of red intensities from minimum frames.\n",
    "    - greenMin: Flattened array of green intensities from minimum frames.\n",
    "    - blueMin: Flattened array of blue intensities from minimum frames.\n",
    "    - videoIndex: Index of the video being processed (used for labeling).\n",
    "\n",
    "    Outputs:\n",
    "    - Displays the histogram plot of minimum and maximum of R, G, B separately per video\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if there is enough data to plot histograms for this video\n",
    "    if len(redMax) == 0 or len(redMin) == 0:\n",
    "        print(f\"Not enough data for Video {videoIndex} to plot histograms.\")\n",
    "        return  # Exit the function if there isn't enough data to plot\n",
    "    \n",
    "    # RGB channel names and their corresponding colors\n",
    "    channels = ['Red', 'Green', 'Blue']  # Channel names for labeling\n",
    "\n",
    "    # Create a figure with 3 subplots (one for each RGB channel)\n",
    "    _, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "    # Loop through each channel (Red, Green, Blue) and plot their histograms\n",
    "    for i, (max_vals, min_vals) in enumerate(zip([redMax, greenMax, blueMax], [redMin, greenMin, blueMin])):\n",
    "        \n",
    "        # Plot histogram for maximum intensity values for the current channel\n",
    "        axs[i].hist(max_vals, bins=50, color='red', alpha=0.5, label=f'Max {channels[i]}', density=True)\n",
    "        \n",
    "        # Plot histogram for minimum intensity values for the current channel\n",
    "        axs[i].hist(min_vals, bins=50, color='green', alpha=0.2, label=f'Min {channels[i]}', density=True)\n",
    "        \n",
    "        # Set title and labels for the current subplot\n",
    "        axs[i].set_title(f\"Video {videoIndex} - {channels[i]} Distribution\")  # Title shows the video index and channel\n",
    "        axs[i].set_xlabel(f\"{channels[i]} Intensity\")  # X-axis label for intensity\n",
    "        axs[i].set_ylabel(\"Frequency\")  # Y-axis label for frequency\n",
    "        \n",
    "        # Display the legend to distinguish between max and min intensities\n",
    "        axs[i].legend()\n",
    "\n",
    "    # Adjust the layout to prevent subplot overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the figure with histograms\n",
    "    plt.show()\n",
    "\n",
    "# Create dictionaries to store maximum and minimum frames and their indices for each video\n",
    "maxFrames = {}\n",
    "minFrames = {}\n",
    "maxIndices = {}\n",
    "minIndices = {}\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Main processing loop for video frames\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# For each video, select frames near maxima and minima and plot histograms\n",
    "for videoIndex in videoFrames:\n",
    "    # Get intensity values for the 5-second chunk\n",
    "    intensityChunk = chunks[videoIndex]\n",
    "    chunkInfo = chunkInfos[videoIndex]\n",
    "\n",
    "    # Select frames near maxima and minima\n",
    "    maxFrame, minFrame, max_index, min_index = select_extreme_frames(intensityChunk)\n",
    "\n",
    "    # Store maxFrames and minFrames for the given videoIndex\n",
    "    maxFrames[videoIndex] = maxFrame\n",
    "    minFrames[videoIndex] = minFrame\n",
    "    maxIndices[videoIndex] = max_index\n",
    "    minIndices[videoIndex] = min_index\n",
    "\n",
    "    # Extract R, G, B values for the selected frames\n",
    "    redMax, greenMax, blueMax = extract_rgb_values_histogram_1D(maxFrame, videoIndex)\n",
    "    redMin, greenMin, blueMin = extract_rgb_values_histogram_1D(minFrame, videoIndex)\n",
    "\n",
    "    # Plot local min and max intensity over time\n",
    "    plot_local_min_max(minIndices, maxIndices, intensityChunk, videoIndex)\n",
    "\n",
    "    # Plot histograms for R, G, B distributions\n",
    "    plot_histograms(redMax, greenMax, blueMax, redMin, greenMin, blueMin, videoIndex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### E. Threshold Based Detection and ROC curve [25 marks]\n",
    "Only consider the \"R\" channel for analysis. Suppose, we just use a single pixel (uniformly randomly chosen in the frame) to detect whether the frame belongs to case 1 or case 2. You can denote the \"Case 1\" to be the POSITIVE event/hypothesis and \"Case 2\" to be the NEGATIVE event/null hypothesis. For every threshold value, for every frame, choose 500 random pixels. Compute the \"Probability of Detection\" ($P_D$) and \"Probability of False Alarm\" ($P_{FA}$). Note that you have (20 + 20) = 40 frames, and 500 detections per frame, i.e., 20000 total detections. Plot the $ROC$ curve. Which one has the best ROC curve (Dataset 1, 2 or 3)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Summary of the Code\n",
    "\n",
    "This code implements a comprehensive analysis of video frames for evaluating a classification algorithm's performance using the Receiver Operating Characteristic (ROC) curve. The steps involved in this analysis are as follows:\n",
    "\n",
    "1. **Function Definitions**:\n",
    "   - **compute_auc(pfa, pd)**: This function calculates the Area Under the Curve (AUC) for the ROC curve using the trapezoidal rule. It accepts two arrays: one for the probabilities of false alarm (PFA) and another for the probabilities of detection (PD). It sorts the PFA values and corresponding PD values, and then computes the AUC based on these sorted arrays.\n",
    "  \n",
    "   - **compute_roc(redPixelsMax, redPixelsMin, thresholds, videoIndex)**: This function computes the true positive rates (Pd) and false positive rates (Pfa) for a given set of pixel values representing maximum and minimum intensity frames. It first fits normal distributions to the pixel values of both maximum and minimum frames, calculates the likelihoods for each hypothesis, and then evaluates the classification performance across specified thresholds. It returns lists of Pd, Pfa, and the corresponding AUC.\n",
    "\n",
    "   - **extract_rgb_values_2D(frames, videoIndex)**: This function extracts the Red, Green, and Blue (RGB) channel values from the selected frames of a video. It returns three lists corresponding to the red, green, and blue channels of the frames.\n",
    "\n",
    "   - **random_pixel_selection(frames, numPixels=500)**: This function randomly selects a specified number of pixels from each frame. It returns a concatenated array of selected pixel values and its row and column index, which will be used in the classification analysis.\n",
    "\n",
    "   - **plot_roc_curves(pd, pfa, auc)**: This function visualizes the ROC curve based on the calculated Pd and Pfa values. It creates a plot that illustrates the trade-off between the true positive rate and the false positive rate, along with a diagonal reference line for random guessing. The AUC score is included in the plot legend.\n",
    "\n",
    "2. **Processing Video Data**:\n",
    "   - The main processing loop iterates through each video in a predefined list (videoFrames). For each video:\n",
    "     - The red channel frames are extracted for both maximum and minimum intensity cases using the extract_rgb_values_2D function.\n",
    "     - A random selection of 500 pixels is made from each frame in both the maximum and minimum cases via the get_red_pixels_max_min function.\n",
    "     - The pixel values are stored in a dictionary for later reference.\n",
    "\n",
    "3. **ROC Curve Calculation**:\n",
    "   - The compute_roc function is called with the selected pixel values and a predefined set of thresholds. This function computes the Pd and Pfa for each threshold and calculates the AUC score.\n",
    "\n",
    "4. **ROC Curve Visualization**:\n",
    "   - The plot_roc_curves function is invoked to visualize the ROC curve for each video, clearly showing the relationship between Pfa and Pd along with the computed AUC score.\n",
    "\n",
    "5. **Output Summary**:\n",
    "   - After processing all videos, the code prints the AUC score for each video. The video with the highest AUC score is identified, indicating the best performance of the classification algorithm across the analyzed video frames.\n",
    "\n",
    "6. **Conclusion**:\n",
    "   - This analysis provides insights into the effectiveness of a pixel-based classification approach by quantifying the performance through ROC curves and AUC scores. It allows for the comparison of different videos to assess which yields the best classification outcomes, ultimately aiding in the evaluation and tuning of video analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## put your snippet here with complete explanation in comments\n",
    "## Marks will be provided based on code readability (not yourself, but others) and detailed comments\n",
    "\n",
    "def compute_auc(pfa, pd):\n",
    "    \"\"\"\n",
    "    Compute the Area Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) curve.\n",
    "    \n",
    "    Parameters:\n",
    "    - pfa: List or array of probabilities of false alarm (PFA).\n",
    "    - pd: List or array of probabilities of detection (PD).\n",
    "\n",
    "    Returns:\n",
    "    - auc: The calculated AUC value, representing the performance of the classifier.\n",
    "    \"\"\"\n",
    "    # Convert input lists to NumPy arrays for easier manipulation\n",
    "    pfa = np.array(pfa)\n",
    "    pd = np.array(pd)\n",
    "\n",
    "    # Sort pfa (Probability of False Alarm) and corresponding pd (Probability of Detection) values to ensure proper calculation\n",
    "    sorted_indices = np.argsort(pfa)  # Get indices that would sort the pfa array\n",
    "    pfa = pfa[sorted_indices]  # Sort pfa\n",
    "    pd = pd[sorted_indices]  # Sort pd according to the sorted pfa\n",
    "\n",
    "    # Calculate Area Under the Curve (AUC) using the trapezoidal rule\n",
    "    auc = np.trapezoid(pd, pfa)  # Numerical integration to find AUC\n",
    "\n",
    "    return auc  # Return the computed AUC\n",
    "\n",
    "def compute_roc(redPixelsMax, redPixelsMin, thresholds, videoIndex):\n",
    "    \"\"\"\n",
    "    Compute the Receiver Operating Characteristic (ROC) curve values including true positive rate (Pd) \n",
    "    and false positive rate (Pfa) based on pixel values from maximum and minimum frames.\n",
    "\n",
    "    Parameters:\n",
    "    - redPixelsMax: Array of pixel values from frames with maximum intensity.\n",
    "    - redPixelsMin: Array of pixel values from frames with minimum intensity.\n",
    "    - thresholds: List of thresholds for classification.\n",
    "    - videoIndex: Index of the video being processed.\n",
    "\n",
    "    Returns:\n",
    "    - pd: List of true positive rates (Pd) for each threshold.\n",
    "    - pfa: List of false positive rates (Pfa) for each threshold.\n",
    "    - auc: Calculated AUC score representing the performance of the classification.\n",
    "    \"\"\"\n",
    "    # Combine the pixel values from maximum and minimum frames\n",
    "    pixelValues = np.concatenate([redPixelsMax, redPixelsMin])\n",
    "\n",
    "    # Fit normal distributions for the maximum and minimum pixel values\n",
    "    maxMu, maxStd = norm.fit(redPixelsMax)  # Mean and standard deviation for max pixel values\n",
    "    minMu, minStd = norm.fit(redPixelsMin)  # Mean and standard deviation for min pixel values\n",
    "\n",
    "    # Calculate likelihoods for both hypotheses\n",
    "    pH0 = norm.pdf(pixelValues, minMu, minStd)  # Likelihood for H0 (negative event)\n",
    "    pH1 = norm.pdf(pixelValues, maxMu, maxStd)  # Likelihood for H1 (positive event)\n",
    "\n",
    "    # Initialize lists to store detection rates (Pd) and false alarm rates (Pfa) for each threshold\n",
    "    pd = []\n",
    "    pfa = []\n",
    "\n",
    "    # Loop through each threshold to compute Pd and Pfa\n",
    "    for threshold in thresholds:\n",
    "        # Likelihood ratio test for classification\n",
    "        classifications = pH1 >= threshold * pH0  # Classify based on the threshold\n",
    "        \n",
    "        # Compute True Positive Rate (Pd) and False Positive Rate (Pfa)\n",
    "        Pd = np.sum(classifications[:len(redPixelsMax)+1]) / len(redPixelsMax)  # Detection rate for positive events\n",
    "        Pfa = np.sum(classifications[-len(redPixelsMin):]) / len(redPixelsMin)  # False alarm rate for negative events\n",
    "\n",
    "        # Store the results in lists\n",
    "        pd.append(Pd)\n",
    "        pfa.append(Pfa)\n",
    "\n",
    "    # Calculate the Area Under the ROC Curve (AUC)\n",
    "    auc = compute_auc(pfa, pd)\n",
    "\n",
    "    return pd, pfa, auc  # Return detection rates, false alarm rates, and AUC\n",
    "\n",
    "def extract_rgb_values_2D(frames, videoIndex):\n",
    "    \"\"\"\n",
    "    Extract the Red, Green, and Blue (RGB) channel frames from the selected frames.\n",
    "    \n",
    "    Parameters:\n",
    "    - frames: List of frames to extract RGB channels from.\n",
    "    - videoIndex: Index of the video being processed.\n",
    "\n",
    "    Returns:\n",
    "    - redFrames: List of Red channel frames.\n",
    "    - greenFrames: List of Green channel frames.\n",
    "    - blueFrames: List of Blue channel frames.\n",
    "    \"\"\"\n",
    "    redFrames = []\n",
    "    greenFrames = []\n",
    "    blueFrames = []\n",
    "\n",
    "    # Loop through each frame to split color channels\n",
    "    for frame in frames:\n",
    "        frameIndex = frame['frameIndex']  # Get the index of the frame\n",
    "        frameTemp = videoFrames[videoIndex][frameIndex]  # Retrieve the actual frame\n",
    "        blue, green, red = cv2.split(frameTemp)  # Separate B, G, R channels\n",
    "        \n",
    "        # Append the red, green, and blue channels to their respective lists\n",
    "        redFrames.append(red)\n",
    "        greenFrames.append(green)\n",
    "        blueFrames.append(blue)\n",
    "\n",
    "    return redFrames, greenFrames, blueFrames  # Return lists of color channel frames\n",
    "\n",
    "def random_pixel_selection(frames, numPixels=500):\n",
    "    \"\"\"\n",
    "    Randomly select a specified number of pixels from each frame.\n",
    "    \n",
    "    Parameters:\n",
    "    - frames: List of frames to select pixels from.\n",
    "    - numPixels: Number of pixels to select from each frame.\n",
    "\n",
    "    Returns:\n",
    "    - selectedPixels: Concatenated array of randomly selected pixel values from all frames.\n",
    "    - selectedPixelsRows: Concatenated array of randomly selected pixel values row index from all frames.\n",
    "    - selectedPixelsCols: Concatenated array of randomly selected pixel values column index from all frames.\n",
    "    \"\"\"\n",
    "    selectedPixels1D = [] # List to store selected pixels\n",
    "    selectedPixelsRows = [] # List to store selected pixels row index\n",
    "    selectedPixelsCols = [] # List to store selected pixels column index\n",
    "\n",
    "    # Loop through each frame\n",
    "    for frame in frames:\n",
    "        # Get the dimensions of the frame (height, width)\n",
    "        height, width = frame.shape\n",
    "\n",
    "        # Randomly select pixels from the flattened array of pixel values\n",
    "        randomIndices = np.random.choice(height * width, numPixels, replace=False)  # Select unique pixel indices\n",
    "        row, col = np.unravel_index(randomIndices, frame.shape) # Select the unique row column value of the selected pixel\n",
    "        selectedPixelsRows.append(row) # Store the pixel row index values for further processing\n",
    "        selectedPixelsCols.append(col) # Store the pixel column index values for further processing\n",
    "        selectedPixels = frame.flatten()[randomIndices] # Extract pixel values using the selected indices\n",
    "        selectedPixels1D.append(selectedPixels)  # Store the pixel values dor further processing\n",
    "\n",
    "    # Return concatenated array of selected pixel values, corresponding row and column index as well\n",
    "    return np.concatenate(selectedPixels1D), np.concatenate(selectedPixelsRows), np.concatenate(selectedPixelsCols)  \n",
    "\n",
    "def plot_roc_curves(pd, pfa, auc, videoIndex):\n",
    "    \"\"\"\n",
    "    Plot the ROC curves based on the given probabilities of detection (Pd) and false alarm (Pfa).\n",
    "    \n",
    "    Parameters:\n",
    "    - pd: List of true positive rates (Pd) for each threshold.\n",
    "    - pfa: List of false positive rates (Pfa) for each threshold.\n",
    "    - auc: The calculated AUC score representing the performance of the classifier.\n",
    "\n",
    "    Outputs:\n",
    "    - Displays the ROC curve per video\n",
    "    \"\"\"\n",
    "    # Dictionary to store plot colors for each video for visualization purposes\n",
    "    plotColors = {\"1\": 'blue', \"2\": 'green', \"3\": 'red'}\n",
    "\n",
    "    # Create a new figure for the ROC curve\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(pfa, pd, linestyle='-', color=plotColors[videoIndex], label=f'Video {videoIndex} (AUC = {auc:.2f})')  # Plot ROC curve\n",
    "    plt.title(\"ROC Curve\")  # Set the title\n",
    "    plt.xlabel(\"Pfa (Probability of False Alarm)\")  # Label for X-axis\n",
    "    plt.ylabel(\"Pd (Probability of Detection)\")  # Label for Y-axis\n",
    "    plt.grid(True)  # Enable grid\n",
    "    plt.legend()  # Display legend\n",
    "\n",
    "    # Add diagonal reference line for random guessing\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.3)  \n",
    "    plt.xlim([0.0, 1.0])  # Set X-axis limits\n",
    "    plt.ylim([0.0, 1.05])  # Set Y-axis limits\n",
    "    plt.xlabel(\"Probability of False Alarm ($P_{FA}$)\")  # X-axis label\n",
    "    plt.ylabel(\"Probability of Detection ($P_{D}$)\")  # Y-axis label\n",
    "    plt.title(f\"Video {videoIndex} - ROC Curve(Red Channel)\")  # Title with video index\n",
    "    plt.legend(loc='lower right')  # Place legend at the lower right corner\n",
    "    plt.grid(True)  # Enable grid\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "# Initialize dictionaries to store AUC scores and PD/PFA values for each video\n",
    "aucScores = {}\n",
    "pdPfaThresholds = {}  # To store PD and PFA values for each video\n",
    "redPixels = {}\n",
    "redPixelsRows = {}\n",
    "redPixelsCols = {}\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Main processing loop for video frames\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Loop through each video to compute and plot ROC curves\n",
    "for index, videoIndex in enumerate(videoFrames):\n",
    "    # Extract R channel frames for maximum and minimum cases\n",
    "    redMax, _, _ = extract_rgb_values_2D(maxFrames[videoIndex], videoIndex)\n",
    "    redMin, _, _ = extract_rgb_values_2D(minFrames[videoIndex], videoIndex)\n",
    "\n",
    "    # Randomly select 500 pixels from maximum and minimum frames\n",
    "    redPixelsMax, redPixelsMaxRows, redPixelsMaxCols = random_pixel_selection(redMax)  # Get random pixels with indices from maximum frames\n",
    "    redPixelsMin, redPixelsMinRows, redPixelsMinCols = random_pixel_selection(redMin)  # Get random pixels with indices from minimum frames\n",
    "\n",
    "    # Store red pixels for maximum and minimum cases\n",
    "    redPixels[videoIndex] = {\"RedPixelsMax\": redPixelsMax, \"RedPixelsMin\": redPixelsMin}\n",
    "    redPixelsRows[videoIndex] = {\"RedPixelsMaxRows\": redPixelsMaxRows, \"RedPixelsMinRows\": redPixelsMinRows}\n",
    "    redPixelsCols[videoIndex] = {\"RedPixelsMaxCols\": redPixelsMaxCols, \"RedPixelsMinCols\": redPixelsMinCols}\n",
    "\n",
    "    # Compute ROC curve with specified thresholds\n",
    "    thresholds = np.arange(0.5, 1.5, 0.1)\n",
    "    pd, pfa, auc = compute_roc(redPixelsMax, redPixelsMin, thresholds, videoIndex)\n",
    "\n",
    "    # Store AUC score for the current video\n",
    "    aucScores[videoIndex] = auc\n",
    "\n",
    "    # Store PD (true positive rate) and PFA (false positive rate) for the current video\n",
    "    pdPfaThresholds[videoIndex] = {\"PD\": pd, \"PFA\": pfa, \"Thresholds\": thresholds}\n",
    "\n",
    "    # Plot the ROC curve for the current video\n",
    "    plot_roc_curves(pd, pfa, auc, videoIndex)\n",
    "\n",
    "# Print AUC scores for each video\n",
    "for videoIndex, aucScore in aucScores.items():\n",
    "    print(f\"Video {videoIndex} AUC Score: {aucScore:.2f}\")\n",
    "\n",
    "# Determine the video with the best AUC score\n",
    "bestVideoId = max(aucScores, key=aucScores.get)  # Get the video index with the highest AUC score\n",
    "print(f\"\\nThe video with the best ROC curve (highest AUC) is Video {bestVideoId} with an AUC score of {aucScores[bestVideoId]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### F. Are \"good\" samples spatially correlated? [20 marks]\n",
    "First, choose an optimal threshold, $T_{OPT}$, that best suits your data (maximize $P_D$ while minimizing $P_{FA}$, you may look into maximizing the $\\frac{P_D}{P_{FA}}$ ratio). Out of the 20000 total detections above for $T_{OPT}$, can it be hypothesised that the \"good\" samples (true positives and true negatives) are spatially clustered in certain areas of the frame, compared to the \"bad\" samples (false positives and false negatives)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Summary\n",
    "\n",
    "This code implements functions for determining the optimal threshold for classification, classifying samples based on that threshold, and visualizing the spatial distribution of classification results. The process involves the following steps:\n",
    "\n",
    "1. **Classification of Samples Using Likelihood Ratio**:\n",
    "   - The classify_samples function classifies red pixel samples into four categories (True Positives, False Negatives, False Positives, and True Negatives) based on a **likelihood ratio test** (LRT). The likelihoods for both positive (Max) and negative (Min) events are calculated using fitted normal distributions, and the classification is determined by comparing the likelihoods with a threshold (TOpt).\n",
    "\n",
    "   1. **True Positives**:\n",
    "      - Samples from redPixelsMax are correctly classified as positive (event detected) when the likelihood of the positive hypothesis (pH1) is greater than or equal to the likelihood of the negative hypothesis (pH0) scaled by the threshold TOpt. These samples represent true detections.\n",
    "      - Condition: pH1 >= TOpt * pH0 for redPixelsMax values.\n",
    "\n",
    "   2. **False Negatives**:\n",
    "      - Samples from redPixelsMax that fail the likelihood test (i.e., pH1 < TOpt * pH0) are classified as false negatives. These samples should have been detected but were missed by the classifier, indicating undetected events.\n",
    "      - Condition: pH1 < TOpt * pH0 for redPixelsMax values.\n",
    "\n",
    "   3. **False Positives**:\n",
    "      - Samples from redPixelsMin are incorrectly classified as positive when the likelihood of the positive hypothesis (pH1) is incorrectly greater than or equal to the likelihood of the negative hypothesis (pH0) scaled by TOpt. These represent background or noise falsely classified as events.\n",
    "      - Condition: pH1 >= TOpt * pH0 for redPixelsMin values.\n",
    "\n",
    "   4. **True Negatives**:\n",
    "      - Samples from redPixelsMin that are correctly classified as negative (i.e., pH1 < TOpt * pH0) indicate that the classifier accurately identified the absence of an event. These samples are non-events or noise that were properly excluded.\n",
    "      - Condition: pH1 < TOpt * pH0 for redPixelsMin values.\n",
    "\n",
    "\n",
    "\n",
    "2. **Sample Classification**:\n",
    "   - The classify_samples function categorizes red pixel values from maximum and minimum intensity frames into four categories based on the optimal threshold: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). This classification allows for an understanding of how well the algorithm performs in distinguishing between events.\n",
    "\n",
    "3. **Spatial Correlation Visualization**:\n",
    "   - The plot_spatial_correlation function generates a 2D scatter plot to visually represent the spatial locations of classified samples. The locations of TP, FN, TN, and FP are plotted in two separate subplots, allowing for a clear visual distinction between correctly and incorrectly classified samples. Each category is represented by a different color.\n",
    "\n",
    "4. **Video Frame Processing**:\n",
    "   - The main loop iterates over each video frame, retrieving relevant data such as PFA, Pd, and thresholds from precomputed dictionaries. For each video, it calculates the optimal threshold, classifies the red pixel samples, and generates random locations for each classification result to plot the spatial correlation.\n",
    "\n",
    "5. **Random Sampling for Visualization**:\n",
    "   - Random pixel locations are generated for each classification category (TP, TN, FP, FN) to ensure that the visual representation does not become cluttered. The code ensures that the number of locations plotted does not exceed the actual number of classified samples.\n",
    "\n",
    "6. **Overall Purpose**:\n",
    "   - The overall purpose of this analysis is to evaluate the performance of a classification algorithm used for video frame analysis. By calculating and visualizing the optimal threshold and its impact on classification, the code provides insights into how well the algorithm can differentiate between true events and noise, helping in the refinement and improvement of video analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## put your snippet here with complete explanation in comments\n",
    "## Marks will be provided based on code readability (not yourself, but others) and detailed comments\n",
    "\n",
    "# Function to find the optimal threshold T_OPT\n",
    "def find_optimal_threshold(pfa, pd, thresholds):\n",
    "    \"\"\"\n",
    "    Calculate the optimal threshold (T_OPT) based on the maximum ratio of \n",
    "    Probability of Detection (Pd) to Probability of False Alarm (Pfa).\n",
    "\n",
    "    Args:\n",
    "        pfa (array-like): Array of probabilities of false alarm.\n",
    "        pd (array-like): Array of probabilities of detection.\n",
    "        thresholds (array-like): Array of threshold values corresponding to Pd and Pfa.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the optimal threshold (TOpt), \n",
    "               maximum probability of detection (maxPd), and minimum probability of false alarm (minPfa).\n",
    "    \"\"\"\n",
    "    # Calculate the PD/PFA ratio for each threshold\n",
    "    # The below process is done to avoid division by zero\n",
    "    non_zero_mask = pfa != 0\n",
    "    pd_pfa_ratios = np.full_like(pfa, np.nan)\n",
    "    pd_pfa_ratios[non_zero_mask] = pd[non_zero_mask] / pfa[non_zero_mask]\n",
    "    pd_pfa_ratios[~non_zero_mask] = 0\n",
    "    # Find the index of the maximum ratio\n",
    "    optimalIdx = np.argmax(pd_pfa_ratios)\n",
    "    \n",
    "    # Get the optimal threshold and corresponding PD and PFA\n",
    "    TOpt = thresholds[optimalIdx]\n",
    "    maxPd = pd[optimalIdx]\n",
    "    minPfa = pfa[optimalIdx]\n",
    "    \n",
    "    return TOpt, maxPd, minPfa\n",
    "\n",
    "# Function to classify samples based on TOpt\n",
    "def classify_samples(redPixelsMax, redPixelsMaxRow, redPixelsMaxCol, redPixelsMin, redPixelsMinRow, redPixelsMinCol, TOpt):\n",
    "    \"\"\"\n",
    "    Classify samples from red pixel values into True Positives (TP), True Negatives (TN),\n",
    "    False Positives (FP), and False Negatives (FN) based on the likelihood ratio test\n",
    "    using the optimal threshold (TOpt).\n",
    "\n",
    "    Args:\n",
    "        redPixelsMax (array-like): Array of maximum red pixel values representing positive samples.\n",
    "        redPixelsMaxRow (array-like): Array of maximum red pixel values row index representing positive samples.\n",
    "        redPixelsMaxCol (array-like): Array of maximum red pixel values column index representing positive samples.\n",
    "        redPixelsMin (array-like): Array of minimum red pixel values representing negative samples.\n",
    "        redPixelsMinRow (array-like): Array of minimum red pixel values row index representing negative samples.\n",
    "        redPixelsMinCol (array-like): Array of minimum red pixel values column index representing negative samples.\n",
    "        TOpt (float): The optimal threshold for classification based on the likelihood ratio.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four lists:\n",
    "               - True Positives (TP): Samples correctly classified as positives(contains the pixel value and its index).\n",
    "               - True Negatives (TN): Samples correctly classified as negatives(contains the pixel value and its index).\n",
    "               - False Positives (FP): Samples incorrectly classified as positives(contains the pixel value and its index).\n",
    "               - False Negatives (FN): Samples incorrectly classified as negatives(contains the pixel value and its index).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine the pixel values from both Max and Min groups\n",
    "    pixelValues = np.concatenate([redPixelsMax, redPixelsMin])\n",
    "    pixelRows = np.concatenate([redPixelsMaxRow, redPixelsMinRow])\n",
    "    pixelCols = np.concatenate([redPixelsMaxCol, redPixelsMinCol])\n",
    "\n",
    "    # Fit normal distributions for the max and min red pixel values\n",
    "    maxMu, maxStd = norm.fit(redPixelsMax)  # Fit for positive (Max) samples\n",
    "    minMu, minStd = norm.fit(redPixelsMin)  # Fit for negative (Min) samples\n",
    "\n",
    "    # Calculate the likelihood for H1 (positive hypothesis) using the Min distribution\n",
    "    pH0 = norm.pdf(pixelValues, minMu, minStd)\n",
    "\n",
    "    # Calculate the likelihood for H0 (negative hypothesis) using the Max distribution\n",
    "    pH1 = norm.pdf(pixelValues, maxMu, maxStd)\n",
    "\n",
    "    # Apply likelihood ratio test: classify based on whether pH1 >= (TOpt * pH0)\n",
    "    classifications = pH1 >= TOpt * pH0\n",
    "\n",
    "    # Initialize lists for storing the classified samples\n",
    "    truePositives = []  # Samples classified as positive and are actually positive\n",
    "    falseNegatives = []  # Samples classified as negative but are actually positive\n",
    "    falsePositives = []  # Samples classified as positive but are actually negative\n",
    "    trueNegatives = []  # Samples classified as negative and are actually negative\n",
    "\n",
    "    # Loop through classifications and assign samples to the appropriate category\n",
    "    for index, cls in enumerate(classifications):\n",
    "        pixelData = {'pixelValue' :pixelValues[index] , 'pixelRow':pixelRows[index] , 'pixelCol':pixelCols[index]}\n",
    "        if(index < len(redPixelsMax)):\n",
    "            if(cls == True):\n",
    "                truePositives.append(pixelData)\n",
    "            else:\n",
    "                falseNegatives.append(pixelData)\n",
    "        else:\n",
    "            if(cls == True):\n",
    "                falsePositives.append(pixelData)\n",
    "            else:\n",
    "                trueNegatives.append(pixelData)\n",
    "    \n",
    "    # Return the classification results as a tuple\n",
    "    return truePositives, trueNegatives, falsePositives, falseNegatives\n",
    "\n",
    "def plot_spatial_correlation(TP, TN, FP, FN, videoIndex):\n",
    "    \"\"\"\n",
    "    Plot the spatial correlation of classified samples in a 2D scatter plot.\n",
    "\n",
    "    Args:\n",
    "        TP (True Positives): Samples correctly classified as positives \n",
    "                             (contains pixel value and its index).\n",
    "        TN (True Negatives): Samples correctly classified as negatives \n",
    "                             (contains pixel value and its index).\n",
    "        FP (False Positives): Samples incorrectly classified as positives \n",
    "                             (contains pixel value and its index).\n",
    "        FN (False Negatives): Samples incorrectly classified as negatives \n",
    "                             (contains pixel value and its index).\n",
    "        frame_shape (tuple): Shape of the frame (height, width) for setting plot limits.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the scatter plot showing spatial correlation of classified samples.\n",
    "    \"\"\"\n",
    "    # Generate random pixel locations for plotting, limiting sample size to 500 or less\n",
    "    # Modify sampleSize to plot more or less number of pixels\n",
    "    sampleSize = 500  # Set fixed sample size for plotting to avoid overcrowding\n",
    "    TP = TP[0:min(sampleSize + 1, len(TP))]  # Subset of True Positives\n",
    "    FN = FN[0:min(sampleSize + 1, len(FN))]  # Subset of False Negatives\n",
    "    TN = TN[0:min(sampleSize + 1, len(TN))]  # Subset of True Negatives\n",
    "    FP = FP[0:min(sampleSize + 1, len(FP))]  # Subset of False Positives\n",
    "\n",
    "    # Extract pixel row and column information for each class (TP, FN, TN, FP)\n",
    "    tpLocations = [(pixelData['pixelRow'], pixelData['pixelCol']) for pixelData in TP]\n",
    "    fnLocations = [(pixelData['pixelRow'], pixelData['pixelCol']) for pixelData in FN]\n",
    "    tnLocations = [(pixelData['pixelRow'], pixelData['pixelCol']) for pixelData in TN]\n",
    "    fpLocations = [(pixelData['pixelRow'], pixelData['pixelCol']) for pixelData in FP]\n",
    "\n",
    "    # Create a figure \n",
    "    fig, axes = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    # Plot True Positives (TP), False Negatives (FN), True Negatives (TN), False Positives (FP)\n",
    "    axes.set_facecolor(\"white\")  # Set background color for clarity\n",
    "\n",
    "    # Plot True Positives (TP) in green if they exist\n",
    "    if tpLocations:\n",
    "        tp_x, tp_y = zip(*tpLocations)  # Unzip row and column positions for scatter plotting\n",
    "        axes.scatter(tp_x, tp_y, color='green', alpha=0.6, label=\"True +ve\")  # Scatter plot for TP\n",
    "\n",
    "    # Plot False Negatives (FN) in orange if they exist\n",
    "    if fnLocations:\n",
    "        fn_x, fn_y = zip(*fnLocations)  # Unzip row and column positions for scatter plotting\n",
    "        axes.scatter(fn_x, fn_y, color='orange', alpha=0.6, label=\"False -ve\")  # Scatter plot for FN\n",
    "\n",
    "    # Plot True Negatives (TN) in blue if they exist\n",
    "    if tnLocations:\n",
    "        tn_x, tn_y = zip(*tnLocations)  # Unzip row and column positions for scatter plotting\n",
    "        axes.scatter(tn_x, tn_y, color='blue', alpha=0.6, label=\"True -ve\")  # Scatter plot for TN\n",
    "\n",
    "    # Plot False Positives (FP) in red if they exist\n",
    "    if fpLocations:\n",
    "        fp_x, fp_y = zip(*fpLocations)  # Unzip row and column positions for scatter plotting\n",
    "        axes.scatter(fp_x, fp_y, color='red', alpha=0.6, label=\"False +ve\")  # Scatter plot for FP\n",
    "\n",
    "    # Set labels, title, and legend\n",
    "    axes.legend(loc=\"upper right\")\n",
    "    axes.set_title(f\"Video {videoIndex} - Spatial Correlation: True +ve, False -ve, True -ve, False +ve\")  # Title showing what is being plotted\n",
    "    axes.set_xlabel(\"Width\")  # X-axis label\n",
    "    axes.set_ylabel(\"Height\")  # Y-axis label\n",
    "\n",
    "    # Adjust layout to avoid overlap between subplots and improve readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the scatter plot with both subplots\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Main processing loop for video frames\n",
    "# --------------------------------------------------------------------------------------------------------------#\n",
    "# Loop through each video to find optiomal threshold and plot the spatial correlation scatter plot\n",
    "for videoIndex in videoFrames:\n",
    "    # Get the frame dimensions for the current video\n",
    "    frameHeight = int(frameHeightValues[videoIndex])  # Extract frame height\n",
    "    frameWidth = int(frameHeightValues[videoIndex])    # Extract frame width\n",
    "\n",
    "    # Get the threshold and corresponding PD (Probability of Detection) and PFA (Probability of False Alarm)\n",
    "    pfa = np.array(pdPfaThresholds[videoIndex][\"PFA\"], dtype=float)  # Convert PFA to a NumPy array\n",
    "    pd = np.array(pdPfaThresholds[videoIndex][\"PD\"], dtype=float)    # Convert PD to a NumPy array\n",
    "    thresholds = pdPfaThresholds[videoIndex][\"Thresholds\"]           # Extract the thresholds for classification\n",
    "    \n",
    "    # Find the optimal threshold (T_OPT) for each video based on PFA and PD\n",
    "    T_OPT, max_pd, min_pfa = find_optimal_threshold(pfa, pd, thresholds)  # Determine T_OPT using the helper function\n",
    "    print(f\"T_Opt {T_OPT}\")  # Print the optimal threshold for debugging\n",
    "\n",
    "    # Get the red pixel max and min values along with their corresponding indices\n",
    "    redPixelsMax = redPixels[videoIndex][\"RedPixelsMax\"]              # Extract maximum red pixel values\n",
    "    redPixelsMaxRow = redPixelsRows[videoIndex][\"RedPixelsMaxRows\"]   # Extract row indices for maximum red pixels\n",
    "    redPixelsMaxCol = redPixelsCols[videoIndex][\"RedPixelsMaxCols\"]   # Extract column indices for maximum red pixels\n",
    "    redPixelsMin = redPixels[videoIndex][\"RedPixelsMin\"]              # Extract minimum red pixel values\n",
    "    redPixelsMinRow = redPixelsRows[videoIndex][\"RedPixelsMinRows\"]   # Extract row indices for minimum red pixels\n",
    "    redPixelsMinCol = redPixelsCols[videoIndex][\"RedPixelsMinCols\"]   # Extract column indices for minimum red pixels\n",
    "    \n",
    "    # Classify the samples based on the optimal threshold (T_OPT)\n",
    "    TP, TN, FP, FN = classify_samples(redPixelsMax, redPixelsMaxRow, redPixelsMaxCol, \n",
    "                                       redPixelsMin, redPixelsMinRow, redPixelsMinCol, T_OPT)\n",
    "\n",
    "    # Plot the spatial correlation of good (TP, TN) and bad (FP, FN) samples\n",
    "    plot_spatial_correlation(TP, TN, FP, FN, videoIndex)  # Visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Submission (report document: 10 marks)\n",
    "##### Deadline: $6^{th}$, October, 2024\n",
    "* Compress the top level directory (ZIP format) containing this notebook with filled-in code along with the ppgvideos folder.\n",
    "* Include a PDF file (10 marks) within the directory, name it \"report.pdf\". Mention your name and roll number.\n",
    "* The report should contain explanations related to the above assignments (A through F), assumptions if any, specific code explanations, algorithms used and inferences made from the plots. Also include references if any.\n",
    "* <b>You MUST not consult your homework code with others</b>. Any plagiarism found in your code (or somebody who is referring to your code) will result in zero credits in this assignment.\n",
    "* Submissions after the deadline will not be entertained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Explicitly free up memory\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
