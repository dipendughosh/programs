{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task 3. Dataset 2 \n",
    "    Input -\n",
    "        Datasets_for_A1 folder needs to be in the current working \n",
    "        directory with the folder structure as \n",
    "        Datasets_for_A1/Regression/Dataset 2/<team number>/\n",
    "        Replace <team number> with the actual number.\n",
    "        e.g. Datasets_for_A1/Regression/Dataset 2/9/\n",
    "    Output -\n",
    "        Linear model for regression using Gaussian basis functions for Datasets 2 and 3\n",
    "        Regularization methods:\n",
    "            1. No regularization\n",
    "            2. Quadratic regularization\n",
    "        Presentation of Results:\n",
    "            ‚Ä¢ Scatter plots with target output on x-axis and model output on y-axis \n",
    "            for the best performing model, for training data and test data.\n",
    "            ‚Ä¢ Tables showing the ùê∏ùëÖùëÄùëÜ on the training data, the validation\n",
    "            data and the test data, for different models.\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lambda_string = \"Œª\"\n",
    "column_names = [\n",
    "    \"Index\",\n",
    "    \"Sample Size\",\n",
    "    \"Œª\",\n",
    "    \"Train Erms\",\n",
    "    \"Validate Erms\",\n",
    "    \"Test Erms\",\n",
    "]\n",
    "table_index = 0\n",
    "table_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "\n",
    "def add_data_to_table(sample_size, lamda, train_erms, validate_erms, test_erms):\n",
    "    row_data = {}\n",
    "    global table_df\n",
    "    global table_index\n",
    "    table_index = table_index + 1\n",
    "    row_data[\"Index\"] = table_index\n",
    "    row_data[\"Sample Size\"] = sample_size\n",
    "    row_data[\"Œª\"] = lamda\n",
    "    row_data[\"Train Erms\"] = train_erms\n",
    "    row_data[\"Validate Erms\"] = validate_erms\n",
    "    row_data[\"Test Erms\"] = test_erms\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        table_df = pd.concat(\n",
    "            [table_df, pd.DataFrame(row_data, index=[0])], ignore_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "def print_table():\n",
    "    print(table_df.to_string(index=False))\n",
    "\n",
    "\n",
    "def get_N(x):\n",
    "    N = len(x)\n",
    "    return N\n",
    "\n",
    "\n",
    "def get_identity_matrix(n):\n",
    "    identity_matrix = np.eye(n)\n",
    "\n",
    "    return identity_matrix\n",
    "\n",
    "def estimate_sigma(x, centroids):\n",
    "    sigma_list = []\n",
    "    # x1=x[0]\n",
    "    # x2=x[1]\n",
    "    # data = np.column_stack((x1, x2))\n",
    "    data = x\n",
    "    N=data.shape[0]\n",
    "    for loop in range(N):\n",
    "        small_phi = []\n",
    "        data_x = data[loop]\n",
    "        # print(\"data_x\",data_x)\n",
    "        for centroid in centroids:\n",
    "            # print(\"centroid\",type(centroid))\n",
    "            # centroidn = np.array(centroid)\n",
    "            # print(\"centroidn\",type(centroidn))\n",
    "            sum = np.square(np.linalg.norm(data_x-centroid,2))\n",
    "            sigma_list.append(sum)\n",
    "    sigma_avg = np.average(sigma_list)\n",
    "\n",
    "    return sigma_avg\n",
    "\n",
    "\n",
    "def get_phi_by_gaussian(x, D):\n",
    "    # D = len(train_x_11) * 0.2\n",
    "    # D = int(D)\n",
    "    # print(len(x))\n",
    "    # print(\"x\",x)\n",
    "    # x1=x[0]\n",
    "    # x2=x[1]\n",
    "    # data = np.column_stack((x1, x2))\n",
    "    data = x\n",
    "    # print(data)\n",
    "    # data = x\n",
    "    cluster_count = D - 1\n",
    "\n",
    "    indices = list(range(data.shape[0]))\n",
    "    # print(\"indices\",indices)\n",
    "    random.shuffle(indices)\n",
    "    centroid_indices = indices[:cluster_count]\n",
    "    # print(\"centroid_indices\",centroid_indices)\n",
    "    centroid_indices = np.array(centroid_indices)\n",
    "    # print(centroid_indices)\n",
    "\n",
    "    centroids = data[centroid_indices]\n",
    "    # print(\"centroids\",centroids)\n",
    "\n",
    "    cluster_members = [[] for _ in range(cluster_count)]\n",
    "\n",
    "    while True:\n",
    "        distances = np.empty((data.shape[0], centroids.shape[0]))\n",
    "        # print(\"distances\",distances)\n",
    "        for i, point in enumerate(data):\n",
    "            for j, centroid in enumerate(centroids):\n",
    "                distance = np.linalg.norm(point-centroid,2)\n",
    "                distances[i, j] = distance\n",
    "\n",
    "        # print(\"distances\",distances)\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for row in distances:\n",
    "            min_distance = float(\"inf\")\n",
    "            min_index = -1\n",
    "            # print(\"min_distance \",min_distance,\" min_index \",min_index)\n",
    "            for i, distance in enumerate(row):\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    min_index = i\n",
    "                    # print(\"min_distance \",min_distance,\" min_index \",min_index)\n",
    "            labels.append(min_index)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # print(\"labels\",labels)\n",
    "\n",
    "        new_centroids = []\n",
    "\n",
    "        for k in range(cluster_count):\n",
    "            cluster_points = data[labels == k]\n",
    "            # print(\"cluster_points\",cluster_points)\n",
    "            centroid = cluster_points.mean(axis=0)\n",
    "            # print(\"centroid\",centroid)\n",
    "            new_centroids.append(centroid)\n",
    "\n",
    "        new_centroids = np.array(new_centroids)\n",
    "\n",
    "        # print(\"centroids\",centroids)\n",
    "        # print(\"new_centroids\",new_centroids)\n",
    "\n",
    "        cluster_members = [[] for _ in range(cluster_count)]\n",
    "        centroid_mapping = {tuple(centroid): [] for centroid in centroids}\n",
    "        for i, label in enumerate(labels):\n",
    "            cluster_members[label].append(data[i])\n",
    "            centroid_mapping[tuple(centroids[label])].append(data[i])\n",
    "        \n",
    "        # print(\"centroid_mapping\",centroid_mapping)\n",
    "        # print(\"===================================================\")\n",
    "        # for centroid, members in centroid_mapping.items():\n",
    "        #     print(f\"Centroid: {centroid}, Cluster Members: {members}\")\n",
    "        # print(\"===================================================\")\n",
    "        \n",
    "\n",
    "\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "        \n",
    "\n",
    "    # print(\"centroid_mapping\",centroid_mapping)\n",
    "    # print(\"centroids\",centroids)\n",
    "    # print(\"centroids\",centroids[0])\n",
    "\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # for centroid, members in centroid_mapping.items():\n",
    "    #     print(f\"Centroid: {centroid}, Cluster Members: {members}\")\n",
    "\n",
    "    # print(\"cluster_members\",cluster_members)\n",
    "    # centroid_colors = plt.cm.Dark2(np.linspace(0, 1, cluster_count+1))\n",
    "    # k=0\n",
    "    # for i, (centroid, members) in enumerate(centroid_mapping.items()):\n",
    "    #     cluster_points = np.array(members)\n",
    "    #     # print(\"cluster_points\",cluster_points)\n",
    "    #     data_x1, data_x2 = cluster_points[:, 0], cluster_points[:, 1]\n",
    "    #     with warnings.catch_warnings():\n",
    "    #         warnings.simplefilter(\"ignore\")\n",
    "    #         plt.scatter(data_x1, data_x2, label=f'Cluster {k+1}',c=centroid_colors[k])\n",
    "    #         # plt.scatter(data_x1, data_x2, label=f'Cluster {k+1}',cmap='Dark2')\n",
    "    #     centroidn = np.array(centroid)\n",
    "    #     centroids_x1, centroids_x2 = centroidn[0], centroidn[1]\n",
    "    #     with warnings.catch_warnings():\n",
    "    #         warnings.simplefilter(\"ignore\")\n",
    "    #         plt.scatter(centroids_x1, centroids_x2, marker='x', s=100, c=centroid_colors[k])\n",
    "    #         # plt.scatter(centroids_x1, centroids_x2, marker='x', s=100, c=centroid_colors[k])\n",
    "    #     k=k+1\n",
    "\n",
    "    # plt.xlabel('x1')\n",
    "    # plt.ylabel('x2')\n",
    "    # plt.title('K-means Clustering')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    ''''\n",
    "    kmean clustering end\n",
    "    '''\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def get_big_phi(x, centroids, sigma):\n",
    "    # big_phi = []\n",
    "    # x1 = x[0]\n",
    "    # x2 = x[1]\n",
    "    # big_phi = get_phi_by_gaussian(x1, x2, D)\n",
    "    # # print(\"design matrix \", big_phi)\n",
    "    # return big_phi\n",
    "\n",
    "    big_phi = []\n",
    "    # for members in cluster_members:\n",
    "    # cluster_points = np.array(members)\n",
    "    # data_x1, data_x2 = cluster_points[:, 0], cluster_points[:, 1]\n",
    "    # print(\"data.shape=\",data.shape)\n",
    "    # x1=x[0]\n",
    "    # x2=x[1]\n",
    "    # data = np.column_stack((x1, x2))\n",
    "    data = x\n",
    "    # print(\"data\",data)\n",
    "    N=data.shape[0]\n",
    "    # print(\"N\",N)\n",
    "    for loop in range(N):\n",
    "        small_phi = []\n",
    "        data_x = data[loop]\n",
    "        # print(\"data_x\",data_x)\n",
    "        for centroid in centroids:\n",
    "            # print(\"centroid\",type(centroid))\n",
    "            # centroidn = np.array(centroid)\n",
    "            # print(\"centroidn\",type(centroidn))\n",
    "            sum = np.square(np.linalg.norm(data_x-centroid,2))\n",
    "            sum = sum / sigma\n",
    "            sum = np.exp(-sum)\n",
    "            small_phi.append(sum)\n",
    "        small_phi = np.insert(small_phi, 0, 1)\n",
    "        big_phi.append(small_phi)\n",
    "\n",
    "    np_big_phi = np.array(big_phi)\n",
    "    # print(\"big_phi\",np_big_phi)\n",
    "\n",
    "    return np_big_phi\n",
    "\n",
    "\n",
    "def get_new_y(w, big_phi, N, D):\n",
    "    new_y = []\n",
    "    loop1 = 0\n",
    "\n",
    "    for loop1 in range(N):\n",
    "        small_phi = big_phi[loop1]\n",
    "        # print(\"small phi \",small_phi)\n",
    "        y = 0\n",
    "        loop2 = 0\n",
    "        for loop2 in range(D):\n",
    "            wi = w[loop2]\n",
    "            small_phii = small_phi[loop2]\n",
    "            y = y + (wi * small_phii)\n",
    "        new_y.append(y)\n",
    "    # print(\"new_y\", new_y)\n",
    "    np_new_y = np.array(new_y)\n",
    "    # print(\"new_y\", np_new_y)\n",
    "    return np_new_y\n",
    "\n",
    "\n",
    "def get_erms(y, t):\n",
    "    loop = 0\n",
    "    total_data = len(y)\n",
    "    # print(\"len y \", len(y), \" len t \", len(t))\n",
    "    sum = 0\n",
    "\n",
    "    for loop in range(total_data):\n",
    "        sum = sum + np.square(y[loop] - t[loop])\n",
    "        # print(\"loop \", loop, \" sum \", sum)\n",
    "\n",
    "    error = sum / total_data\n",
    "\n",
    "    erms = np.sqrt(error)\n",
    "\n",
    "    return erms\n",
    "\n",
    "\n",
    "def scatter_plot_2d(x, y, x_label, y_label, plot_title, plot_color):\n",
    "    plt.scatter(x, y, color=plot_color, label=plot_title)\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def plot_scatter(\n",
    "    train_y, new_train_y, val_y, new_val_y, test_y, new_test_y, title, sample_size, lamda\n",
    "):\n",
    "    plot_title = \"Train Data Scatter Plot\"\n",
    "    scatter_plot_2d(train_y, new_train_y, \"Actual\", \"Predicted\", plot_title, \"blue\")\n",
    "\n",
    "    plot_title = \"Validation Data Scatter Plot\"\n",
    "    scatter_plot_2d(val_y, new_val_y, \"Actual\", \"Predicted\", plot_title, \"green\")\n",
    "\n",
    "    plot_title = \"Test Data Scatter Plot\"\n",
    "    scatter_plot_2d(test_y, new_test_y, \"Actual\", \"Predicted\", plot_title, \"red\")\n",
    "\n",
    "    plot_title = title + \"ScatterPlot - SampleSize=\" + str(sample_size) + \" Œª=\" + str(lamda)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def my_pseudo_inverse(matrix):\n",
    "    # print(\"matrix\",matrix)\n",
    "    matrix_transpose = np.transpose(matrix)\n",
    "    # print(\"matrix_transpose\",matrix_transpose)\n",
    "    phi_trans_multi_phi = np.dot(matrix_transpose, matrix)\n",
    "    # print(\"phi_trans_multi_phi\",phi_trans_multi_phi)\n",
    "    phi_trans_multi_phi_inv = np.linalg.inv(phi_trans_multi_phi)\n",
    "    # print(\"phi_trans_multi_phi_inv\",phi_trans_multi_phi_inv)\n",
    "    phi_trans_multi_phi_inv_multi_phi_trans = np.dot(\n",
    "        phi_trans_multi_phi_inv, matrix_transpose\n",
    "    )\n",
    "\n",
    "    return phi_trans_multi_phi_inv_multi_phi_trans\n",
    "\n",
    "\n",
    "def my_pseudo_inverse_with_regularization(matrix, matrix_dimension, lamda):\n",
    "    matrix_transpose = np.transpose(matrix)\n",
    "    phi_trans_multi_phi = np.dot(matrix_transpose, matrix)\n",
    "    Identity = get_identity_matrix(matrix_dimension)\n",
    "    regularization_matrix = np.multiply(Identity, lamda)\n",
    "    phi_trans_multi_phi_add_regularized = np.add(\n",
    "        phi_trans_multi_phi, regularization_matrix\n",
    "    )\n",
    "    phi_trans_multi_phi_inv = np.linalg.inv(phi_trans_multi_phi_add_regularized)\n",
    "    phi_trans_multi_phi_inv_multi_phi_trans = np.dot(\n",
    "        phi_trans_multi_phi_inv, matrix_transpose\n",
    "    )\n",
    "\n",
    "    return phi_trans_multi_phi_inv_multi_phi_trans\n",
    "\n",
    "\n",
    "def my_polyfit(big_phi, y, D, lamda=0.0):\n",
    "    if lamda == 0.0:\n",
    "        # pseudo_inv_np_big_phi = np.linalg.pinv(np_big_phi)\n",
    "        # print(\"pseudo inverse design matrix \", pseudo_inv_np_big_phi)\n",
    "        my_pseudo_inv_np_big_phi = my_pseudo_inverse(big_phi)\n",
    "        # print(\"my pseudo inverse design matrix \", my_pseudo_inv_np_big_phi)\n",
    "        pseudo_inv_np_big_phi = my_pseudo_inv_np_big_phi\n",
    "    else:\n",
    "        # pseudo_inv_np_big_phi = np.linalg.inv(np_big_phi.T @ np_big_phi + lamda * np.eye(np_big_phi.shape[1])) @ np_big_phi.T\n",
    "        # print(\"pseudo inverse design matrix \", pseudo_inv_np_big_phi)\n",
    "        my_regularized_pseudo_inv_np_big_phi = my_pseudo_inverse_with_regularization(\n",
    "            big_phi, D, lamda\n",
    "        )\n",
    "        # print(\"my regularized pseudo inverse design matrix \", my_regularized_pseudo_inv_np_big_phi)\n",
    "        pseudo_inv_np_big_phi = my_regularized_pseudo_inv_np_big_phi\n",
    "\n",
    "    w = np.dot(pseudo_inv_np_big_phi, y)\n",
    "    # print(\"w \", w)\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def plot_graphs(train_x, train_y, title, val_x, val_y, test_x, test_y, lamda=0.0):\n",
    "    # column_names = ['Sample Size', 'Degree', 'Œª', 'Train Erms', 'Validate Erms', 'Test Erms']\n",
    "    # table_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    N = get_N(train_y)\n",
    "    sample_size = N\n",
    "    D = N * 0.2\n",
    "    D = int(D)\n",
    "    # print(\"D\",D)\n",
    "    centroids = get_phi_by_gaussian(train_x, D)\n",
    "    # print(\"centroids\",centroids)\n",
    "\n",
    "    sigma = estimate_sigma(train_x, centroids)\n",
    "    # print(\"sigma=\",sigma)\n",
    "\n",
    "    # print(\"N = \", N, \" d = \", d, \" M = \", M, \" D = \", D)\n",
    "    big_phi_train = get_big_phi(train_x, centroids, sigma)\n",
    "    # print(\"big_phi_train\",big_phi_train)\n",
    "    w = my_polyfit(big_phi_train, train_y, D, lamda)\n",
    "    # print(\"w \", w)\n",
    "\n",
    "    # print(x1x2)\n",
    "    # print(new_train_y)\n",
    "    new_train_y = get_new_y(w, big_phi_train, N, D)\n",
    "    train_erms = get_erms(new_train_y, train_y)\n",
    "    # print(\"Erms(sample size = \", N, \" degree = \", M, \" lambda = \",regularization_coefficient,\") train_erms = \",train_erms)\n",
    "\n",
    "    N = get_N(val_y)\n",
    "    big_phi_val = get_big_phi(val_x, centroids, sigma)\n",
    "    new_val_y = get_new_y(w, big_phi_val, N, D)\n",
    "    validate_erms = get_erms(new_val_y, val_y)\n",
    "    # print(\"Erms(sample size = \", N, \" degree = \", M, \" lambda = \",regularization_coefficient,\") validate_erms = \",validate_erms)\n",
    "\n",
    "    N = get_N(test_y)\n",
    "    big_phi_test = get_big_phi(test_x, centroids, sigma)\n",
    "    new_test_y = get_new_y(w, big_phi_test, N, D)\n",
    "    test_erms = get_erms(new_test_y, test_y)\n",
    "    # print(\"Erms(sample size = \", N, \" degree = \", M, \" lambda = \",regularization_coefficient,\") test_erms = \",test_erms)\n",
    "\n",
    "    add_data_to_table(sample_size, lamda, train_erms, validate_erms, test_erms)\n",
    "\n",
    "    plot_scatter(\n",
    "        train_y, new_train_y, val_y, new_val_y, test_y, new_test_y, title, sample_size, lamda\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    folder_number = \"9\"\n",
    "    current_directory = os.getcwd()\n",
    "    # regression_dataset_2_path=current_directory+ \"/Datasets_for_A1/Regression/Dataset 2/\" + folder_number + \"/\"\n",
    "    regression_dataset_2_path = (\n",
    "        \"/home/dipendu/programs/mtech_2023/ml/ass1/Datasets_for_A1/Regression/Dataset 2/\"\n",
    "        + folder_number\n",
    "        + \"/\"\n",
    "    )\n",
    "\n",
    "    regression_dataset_2_Train_Sample_1 = (\n",
    "        regression_dataset_2_path + \"train50_\" + folder_number + \".csv\"\n",
    "    )\n",
    "    # df = pd.read_csv(regression_dataset_2_Train_Sample_1)\n",
    "    # train_x_11 = np.squeeze(np.asanyarray(df[[\"x1\"]]))\n",
    "    # train_x_12 = np.squeeze(np.asanyarray(df[[\"x2\"]]))\n",
    "    # train_y_1 = np.squeeze(np.asanyarray(df[[\"y\"]]))\n",
    "    # data = list(zip(train_x_11, train_x_12, train_y_1))\n",
    "    # sorted_data = sorted(data, key=lambda x: x[0])\n",
    "    # train_x_11, train_x_12, train_y_1 = zip(*sorted_data)\n",
    "    # train_x_1 = []\n",
    "    # train_x_1.append(train_x_11)\n",
    "    # train_x_1.append(train_x_12)\n",
    "    df = pd.read_csv(regression_dataset_2_Train_Sample_1)\n",
    "    data = df.to_numpy()\n",
    "    train_x_1 = data[:, :2]\n",
    "    train_y_1 = data[:, 1]\n",
    "\n",
    "    regression_dataset_2_Train_Sample_2 = (\n",
    "        regression_dataset_2_path + \"train200_\" + folder_number + \".csv\"\n",
    "    )\n",
    "    # df = pd.read_csv(regression_dataset_2_Train_Sample_2)\n",
    "    # train_x_21 = np.squeeze(np.asanyarray(df[[\"x1\"]]))\n",
    "    # train_x_22 = np.squeeze(np.asanyarray(df[[\"x2\"]]))\n",
    "    # train_y_2 = np.squeeze(np.asanyarray(df[[\"y\"]]))\n",
    "    # data = list(zip(train_x_21, train_x_22, train_y_2))\n",
    "    # sorted_data = sorted(data, key=lambda x: x[0])\n",
    "    # train_x_21, train_x_22, train_y_2 = zip(*sorted_data)\n",
    "    # train_x_2 = []\n",
    "    # train_x_2.append(train_x_21)\n",
    "    # train_x_2.append(train_x_22)\n",
    "    df = pd.read_csv(regression_dataset_2_Train_Sample_2)\n",
    "    data = df.to_numpy()\n",
    "    train_x_2 = data[:, :2]\n",
    "    train_y_2 = data[:, 1]\n",
    "\n",
    "    regression_dataset_2_validation = (\n",
    "        regression_dataset_2_path + \"val_\" + folder_number + \".csv\"\n",
    "    )\n",
    "    # df = pd.read_csv(regression_dataset_2_validation)\n",
    "    # val_x_1 = np.squeeze(np.asanyarray(df[[\"x1\"]]))\n",
    "    # val_x_2 = np.squeeze(np.asanyarray(df[[\"x2\"]]))\n",
    "    # val_y = np.squeeze(np.asanyarray(df[[\"y\"]]))\n",
    "    # data = list(zip(val_x_1, val_x_2, val_y))\n",
    "    # sorted_data = sorted(data, key=lambda x: x[0])\n",
    "    # val_x_1, val_x_2, val_y = zip(*sorted_data)\n",
    "    # val_x = []\n",
    "    # val_x.append(val_x_1)\n",
    "    # val_x.append(val_x_2)\n",
    "    df = pd.read_csv(regression_dataset_2_validation)\n",
    "    data = df.to_numpy()\n",
    "    val_x = data[:, :2]\n",
    "    val_y = data[:, 1]\n",
    "\n",
    "    regression_dataset_2_test = (\n",
    "        regression_dataset_2_path + \"test_\" + folder_number + \".csv\"\n",
    "    )\n",
    "    # df = pd.read_csv(regression_dataset_2_test)\n",
    "    # test_x_1 = np.squeeze(np.asanyarray(df[[\"x1\"]]))\n",
    "    # test_x_2 = np.squeeze(np.asanyarray(df[[\"x2\"]]))\n",
    "    # test_y = np.squeeze(np.asanyarray(df[[\"y\"]]))\n",
    "    # data = list(zip(test_x_1, test_x_2, test_y))\n",
    "    # sorted_data = sorted(data, key=lambda x: x[0])\n",
    "    # test_x_1, test_x_2, test_y = zip(*sorted_data)\n",
    "    # test_x = []\n",
    "    # test_x.append(test_x_1)\n",
    "    # test_x.append(test_x_2)\n",
    "    df = pd.read_csv(regression_dataset_2_test)\n",
    "    data = df.to_numpy()\n",
    "    test_x = data[:, :2]\n",
    "    test_y = data[:, 1]    \n",
    "\n",
    "\n",
    "    regularization_coefficients = [0.0, 0.0001, 1e-9, 1e-18]\n",
    "    #regularization_coefficients = [0.0]\n",
    "\n",
    "    for lamda in regularization_coefficients:\n",
    "        plot_graphs(\n",
    "            train_x_1,\n",
    "            train_y_1,\n",
    "            \"train50_\" + folder_number,\n",
    "            val_x,\n",
    "            val_y,\n",
    "            test_x,\n",
    "            test_y,\n",
    "            lamda,\n",
    "        )\n",
    "\n",
    "    for lamda in regularization_coefficients:\n",
    "        plot_graphs(\n",
    "            train_x_2,\n",
    "            train_y_2,\n",
    "            \"train200_\" + folder_number,\n",
    "            val_x,\n",
    "            val_y,\n",
    "            test_x,\n",
    "            test_y,\n",
    "            lamda,\n",
    "        )\n",
    "\n",
    "    print_table()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
